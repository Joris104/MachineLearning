{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early testing pipeline\n",
    "\n",
    "The purpose of this pipeline is to test different features compared to a baseline. <br>\n",
    "<br>\n",
    "The baseline are the features given for the first phase. The different extract_features are written in different .py files that are here imported. <br>\n",
    "The different features tested already are:\n",
    "- Drop z-coordinates and only use xy-coordinates.\n",
    "- Look at movement vectors between different frames.\n",
    "- Frames averaging.\n",
    "- Transforming coordinates to polar.\n",
    "- Separate frames in different ROI (regions are different body parts).\n",
    "- Layered average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of features\n",
    "\n",
    "The baseline feature set achieves the following scores: <br>\n",
    "- train accuracy: 81%<br>\n",
    "- cross-validation accuracy: 62%<br>\n",
    "\n",
    "For quick overview of tested features:\n",
    "- cartesian average (2 frames average):\n",
    "    - train accuracy: 84%\n",
    "    - cross-validation accuracy: 66% \n",
    "\n",
    "- polar coordinates (2 frames average):\n",
    "    - train accuracy: 81%<br>\n",
    "    - cross-validation accuracy: 65%<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
    "\n",
    "import utils_for_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "train_samples = utils_for_students.load_dataset_stage2('data/stage2_labels_train.csv', 'train')\n",
    "test_samples = utils_for_students.load_dataset_stage2('data/stage2_ids_test.csv', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change only the following cell!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the file from which you import for testing different feature extraction\n",
    "\n",
    "#from baseline import extract_features\n",
    "#from cartesian import extract_features\n",
    "#from polar import extract_features\n",
    "from layered_avg import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the training set features.\n",
    "X_train = []\n",
    "y_train = []\n",
    "signers_train = []\n",
    "for sample in train_samples:\n",
    "    pose_sequence = utils_for_students.load_sample_stage2(os.path.join('data/stage2/train/', sample['path']))\n",
    "    X_train.append(extract_features(pose_sequence, s=10, m=10))\n",
    "    y_train.append(sample['label'])\n",
    "    signers_train.append(sample['signer'])\n",
    "    \n",
    "# Concatenate the test set features.\n",
    "X_test = []\n",
    "test_ids = []\n",
    "for sample in test_samples:\n",
    "    pose_sequence = utils_for_students.load_sample_stage2(os.path.join('data/stage2/test/', sample['path']))\n",
    "    X_test.append(extract_features(pose_sequence, s=10, m=10))\n",
    "    test_ids.append(sample['id'])\n",
    "\n",
    "#Combining to numpy array\n",
    "X_train = np.stack(X_train)\n",
    "X_test = np.stack(X_test)\n",
    "\n",
    "# Encode the labels as integers\n",
    "label_encoder = utils_for_students.label_encoder()\n",
    "y_train = label_encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2191, 3750)\n",
      "(541, 3750)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectFwe, SelectFromModel\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#chosen PCA instead of LDA as the n_components of LDA has to be <= min(n_classes - 1, n_features) which is 14 in\n",
    "#this case (down from 750). Which is likely to little given the that the features are only x, y or z values.\n",
    "preprocessing = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('decompose', PCA()),\n",
    "    ('rescaler', StandardScaler())\n",
    "                        ]) \n",
    "\n",
    "#TODO: define feature selection pipeline here\n",
    "#first we remove the features that might lead to false results\n",
    "#then we use selectFromModel to assign weights and take the least important features away for generalization\n",
    "#ofcourse only using linear regression models (the same model as the actual classifier)\n",
    "feature_selection = Pipeline([\n",
    "    ('selectFromModel', SelectFromModel(LogisticRegression(C=1.0e-5, max_iter=10000))),\n",
    "    ('familyWiseError', SelectFwe())\n",
    "                            ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(fit_intercept=False, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param grid has been set to some constants because this is not subject of optimization in this notebook\n",
    "param_grid = {\n",
    "    'feature_selection__familyWiseError__alpha' : [0.75],\n",
    "    'feature_selection__selectFromModel__threshold': [\"1.25*median\"],\n",
    "    'classifier__C': [1.0e-6],\n",
    "    'classifier__tol': [1.0e-4],\n",
    "    'classifier__class_weight': [None]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 4\n",
    "\n",
    "# The function below is just an example!\n",
    "#TODO: write a better split function here?\n",
    "#split according to signer for i.i.d. sets\n",
    "def create_folds(X,y,n_folds):\n",
    "    folds = []\n",
    "    cv_object = StratifiedGroupKFold(n_splits = n_folds)\n",
    "    for (train_indices, val_indices) in  cv_object.split(X_train, y_train, groups=signers_train):\n",
    "        folds.append((train_indices,val_indices))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)])\n",
    "\n",
    "folds = create_folds(X_train,y_train,n_folds)\n",
    "assert isinstance(folds,list),'Folds must be presented as tuples of train and test index lists' \n",
    "\n",
    "# train model\n",
    "cv = GridSearchCV(pipeline, param_grid, n_jobs=4, cv=folds, verbose=1, return_train_score=True, refit=True)\n",
    "cv.fit(X_train, y_train)\n",
    "    \n",
    "prediction = utils_for_students.label_encoder().inverse_transform(cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Printing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.7882271562598058 +/- 0.023033425867872954\n",
      "Cross-validation accuracy: 0.5491564890019562 +/- 0.03661186114151486\n",
      "Best estimator:\n",
      "Pipeline(steps=[('preprocessing',\n",
      "                 Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                 ('decompose', PCA()),\n",
      "                                 ('rescaler', StandardScaler())])),\n",
      "                ('feature_selection',\n",
      "                 Pipeline(steps=[('selectFromModel',\n",
      "                                  SelectFromModel(estimator=LogisticRegression(C=1e-05,\n",
      "                                                                               max_iter=10000),\n",
      "                                                  threshold='1.25*median')),\n",
      "                                 ('familyWiseError', SelectFwe(alpha=0.75))])),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, fit_intercept=False,\n",
      "                                    max_iter=1000))])\n"
     ]
    }
   ],
   "source": [
    "results = cv.cv_results_\n",
    "mean_train_score = results['mean_train_score'][cv.best_index_]\n",
    "std_train_score = results['std_train_score'][cv.best_index_]\n",
    "mean_cv_score = results['mean_test_score'][cv.best_index_]\n",
    "std_cv_score = results['std_test_score'][cv.best_index_]\n",
    "\n",
    "print('Training accuracy {} +/- {}'.format(mean_train_score, std_train_score))\n",
    "print('Cross-validation accuracy: {} +/- {}'.format(mean_cv_score, std_cv_score))\n",
    "\n",
    "print('Best estimator:')\n",
    "print(cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
