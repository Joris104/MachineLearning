{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Notebook\n",
    "\n",
    "This notebook illustrates how to use the datasets provided to you and will be required to submit your first model.\n",
    "\n",
    "## 0. Dependencies\n",
    "\n",
    "This notebook requires several Python **3** packages, which are included in Anaconda 3 for Python 3.8, which is the python distribution we recommend you to use throughout this course.\n",
    "The package versions listed below have been used for testing and are confirmed to work well.\n",
    "We strongly recommend you to install these specific versions to ensure this notebook (and associated autograding) works as expected and we can offer you optimal support throughout the competition  \n",
    "\n",
    "python: 3.8\n",
    "\n",
    "scikit-learn: 1.0.0\n",
    "\n",
    "numpy: 1.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.0.0 in /home/miguel/.local/lib/python3.8/site-packages (1.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.19.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/miguel/.local/lib/python3.8/site-packages (from scikit-learn==1.0.0) (1.7.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/miguel/.local/lib/python3.8/site-packages (from scikit-learn==1.0.0) (3.0.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/miguel/.local/lib/python3.8/site-packages (from scikit-learn==1.0.0) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.0 numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to use this Notebook\n",
    "\n",
    "This Notebook is intended as an introduction to the course and will show you how to load the data and guide you towards training your first model in scikit-learn and making a submission on kaggle and Ufora. \n",
    "Throughout the notebook there are several portions marked with **Action required** where you will be asked to complete missing parts in order to finish this assignment.\n",
    "After you have completed all necessary steps, this notebook will generate a CSV (.csv) file to be submitted on the [Kaggle competition page](https://www.kaggle.com/c/ugentml21-slc-1/)\n",
    "\n",
    "Furthermore, your model will be saved in a Pickle (.pkl) file, which, together with this filled-out Notebook, you have to submit to [Ufora](https://ufora.ugent.be/d2l/home/446146) \n",
    " \n",
    "**In order to ensure your submission will be suitable for our autograding system, some parts of this notebook have been locked and are not editable in order to avoid students editing them by mistake.**\n",
    "\n",
    "**Please do not unlock these cells and edit them on purpose, as this might break our autograding system.\n",
    "Since it is not feasible for us to grade >130 assignments by hand, submissions that can not be autograded will generally graded with 0 points.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your personal data in the fields below. This will not be used during grading, but just to give your submission and model files a meaningful name. Also you may change the prefix of the submission CSV file or append timestamps to your saved models in order to keep them apart when trying out several things with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your data, used to name the output file\n",
    "student_id = \"01703327\"\n",
    "student_lastname = \"Vercouter\" \n",
    "student_firstname = \"Ward\"\n",
    "\n",
    "# change this if you would like your submission outputfile to have a more detailed name, e.g. submission_with_special_preprocessing \n",
    "submission_prefix='submission'\n",
    "\n",
    "# whether or not you want your created models and submissions versioned using timestamps\n",
    "# (setting this to False will overwrite previously exported model and submission files of the same name)\n",
    "use_timestamps = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data\n",
    "\n",
    "The dataset contains videos of people signing in flemish sign language (Vlaamse Gebarentaal). It consists of 15 classes corresponding to lexical signs. From these videos, 3D keypoints were extracted using MediaPipe Holistic. In total, there are 125 keypoints, resulting in 375 (=3x125) floating point values per video frame.\n",
    "\n",
    "For this first stage though, in order for you to focus on building a proper pipeline, we have precomputed a set of simple features, ready for you to use. These are the time averages of all keypoint coordinates over the first and the second half of the sample frames, so 750 features in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries we need: \n",
    "- sklearn and numpy to do machine learning, \n",
    "- csv and pickle read the data and write out submission and model files, \n",
    "- time and os to keep organized with the files we output.\n",
    "We also import some specific sklearn components as well as an utils library with some handy extra functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_for_students'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-79dab45c6347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils_for_students\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_for_students'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "import utils_for_students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our utils_for_students library to load the data from disk. Remember to put the [unzipped files from the competition page](https://www.kaggle.com/c/ugentml21-slc-1/data) into the right paths on your filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "train_samples = utils_for_students.load_dataset_stage1('data/stage1_features/train.csv', 'train')\n",
    "test_samples = utils_for_students.load_dataset_stage1('data/stage1_features/test.csv', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our train data, we get a list of python dictionaries, where each dictionary corresponds to one sign language clip, indicating its feature vector (which we want to present to the model), its labels (the intended output of our model) as well as which person is signing on this clip (think why this information could be interesting?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_samples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our test data, we only receive features, no labels, as the model is supposed to infer them. There is also no signer information in the test data: since your model is expected to generalise to unseen signers, it should also not use signer identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_samples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we concatenate data and labels, and also keep all our signers in a list, they might come in handy, who knows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the training set features.\n",
    "X_train = []\n",
    "y_train = []\n",
    "signers_train = []\n",
    "for sample in train_samples:\n",
    "    X_train.append(sample['features'])\n",
    "    y_train.append(sample['label'])\n",
    "    signers_train.append(sample['signer'])\n",
    "    \n",
    "# Concatenate the test set features.\n",
    "X_test = []\n",
    "test_ids = []\n",
    "for sample in test_samples:\n",
    "    X_test.append(sample['features'])\n",
    "\n",
    "#Combining to numpy array\n",
    "X_train = np.stack(X_train)\n",
    "X_test = np.stack(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "For stage 1, we have performed feature extraction for you, the matrices constructed in the previous cell already contain the extracted features.\n",
    "\n",
    "We extracted these features by splitting every sequence of extracted 3d keypoints from the sign language video into 3 segments of equal duration.\n",
    "Then, we extracted per segment the average positions of each keypoint (375 values). The result is 750 features per sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Action required: Creating pipelines for preprocessing and feature selection \n",
    "Now that we have loaded train and test features, we need to define pipeline steps to preprocess our data and select good features for our model. While in later stages of the competition you will be free to train models with sklearn in accordance to your preferred coding style, for now we would like you to strictly adhere to our predefined structure using the [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline) and [`GridsearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) classes of scikit learn.\n",
    "\n",
    "In a pipeline object you list all sklearn modules which you would like to be applied one by one to your features. This pipeline object is then handed to `GridsearchCV` in order to find good hyperparameters. It will also be your job to decide which hyperparameters need to be optimised an which values for each parameter need to be explored.\n",
    "\n",
    "Let's start with the pipeline though. In this assignment, we ask you to identify two sub-pipelines with fixed names: one for preprocessing and one for feature selection. \n",
    "\n",
    "Each of these can takes a list of \\[name\\]-\\[value\\] tuples where \\[name\\] indicates the name of module and \\[value\\] is the corresponding sklearn object. As you will see a bit later in the code below, it is possible to construct a pipeline out of other pipelines. This is exactly how we will combine our preprocessing and feature selection pipelines, together with the model, into the final pipeline later.\n",
    "\n",
    "Feel free to read forward to step 8 to see how preprocessing and feature extraction pipelines as well as the classifier are used in combination with `GridsearchCV`, to get a more comprehensive picture on how these will be used.\n",
    "\n",
    "For possible candidates for preprocessing and feature selection modules, see [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) and [sklearn.feature_selection](https://scikit-learn.org/stable/modules/classes.html?highlight=feature_selection#module-sklearn.feature_selection)\n",
    "\n",
    "**Warning: these are not exhaustive lists. There may be modules in other namespaces suitable for preprocessing, as well as modules in this namespace unsuitable for the task at hand**\n",
    "\n",
    "**Warning: often, pipeline modules have hyperparameters. It is always advised to carefully read the documentation to decide whether or not it may be advised to optimise those.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define preprocessing pipeline here\n",
    "# It is up to you to define the number of modules in each pipeline and their types\n",
    "# Choose meaningful names for your modules\n",
    "# DO NOT change the names of the pipelines themselves (i.e., \"preprocessing\" and \"feature_selection\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "preprocessing = Pipeline([('scale',MinMaxScaler())]) \n",
    "#TODO: define feature selection pipeline here\n",
    "feature_selection = Pipeline([('selector',SelectKBest(chi2))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Action required: define suitable classifier\n",
    "With your preprocessing and feature selection in place, it is now time to define teh final element: a suitable linear classifier. \n",
    "See [sklearn.linear_model](https://scikit-learn.org/stable/modules/classes.html?highlight=feature_selection#module-sklearn.linear_model) for models and their interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#TODO: define proper classifier\n",
    "classifier = LogisticRegression(class_weight='balanced',solver='lbfgs', multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Action required: Set up hyperparameter grid for [GridsearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) object.\n",
    "\n",
    "GridsearchCV takes hyperparameter lists as a dictionary, where each key is the fully qualified name of the hyperparameter in the Pipeline, and the value is a list of hyperparameter values to be evaluated.\n",
    "\n",
    "In the sklearn example notebook, we used a single pipeline, \n",
    "we addressed the parameter(s) in that pipeline by using `<component_name>__<parameter_name>` (note the double underscore). Here, we extend the example of the notebook to show how it looks for two tuned parameters:\n",
    "\n",
    "`tuned_parameters = [{'logreg__C': [0,0001,0.001,0.01,0.1,1.0],,'logreg__class_weight':['balanced',None]}]`\n",
    "\n",
    "In the current notebook, we are using separate pipelines for preprocessing and feature selection and combine these with the model into a final pipeline. For the first two, the parameter names need to be extended to `<pipeline_name>__<component_name>__<parameter_name>`. \n",
    "\n",
    "The field below shows how this could look if your preprocessing and features selection pipelines consist of 2 modules each and you tune two parameters in each of those pipelines + two parameters for the. Now you need to adapt this with what you decided for your pipeline. \n",
    "   \n",
    "If this still seems confusing to you, feel free to read forward to step 8 to see how preprocessing and feature extraction pipelines as well as the classifier are used in combination with `GridsearchCV`, which should clear things up. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'feature_selection__selector__k' : [490,500,510],\n",
    "                'classifier__C' : [9*10**4,10*10**4,11*10**4],\n",
    "                'classifier__max_iter' : [3000,4000,5000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Action required: Define the number of crossvaldation folds and how to split\n",
    "\n",
    "Time to fix the crossvalidation parameters. Indicate how many crossvaldation folds should be used by setting the n_folds variable.\n",
    "Furthermore, as you'll learn in the lecture, splitting your data correctly when doing crossvalidation is very important. The code in the function `create_folds` below shows a very basic random splitting strategy using sklearns [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) splitter object.\n",
    "Consider whether a better splitting strategy would be possible/necessary given the data you have received. If so, implement this better splitting strategy in `create_folds`.\n",
    "\n",
    "Note that, while `GridsearchCV` objects can handle `KFold` splitters as the one used below, for this stage, we require you to return the splits as a list of tuples of train and test indices for each fold in order to enable autograding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: set appropriate number of cv folds\n",
    "n_folds = 4\n",
    "\n",
    "# The function below is just an example!\n",
    "#TODO: write a better split function here?\n",
    "def create_folds(X,y,n_folds):\n",
    "    folds = []\n",
    "    cv_object = KFold(n_splits = n_folds)\n",
    "    for (train_indices, val_indices) in  cv_object.split(X_train, y_train):\n",
    "        folds.append((train_indices,val_indices))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the model (Locked Cell)\n",
    "Now it is time to put everything togehter and train the model. As you can see, `GridsearchCV` takes the pipelines as well as the classifier and the hyperparameter dictionary you defined, and uses `create_folds` to create list of train and test indices for each split. Then the model is trained using `cv.fit()` and the model and submission files are written to the file system.\n",
    "\n",
    "**You will notice that this cell is locked to avoid editing by mistake. Please do not edit it or split it, and submit the model and submission file generated by this code in order to make sure your work can be autograded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('classifier', classifier)])\n",
    "\n",
    "folds = create_folds(X_train,y_train,n_folds)\n",
    "assert isinstance(folds,list),'Folds must be presented as tuples of train and test index lists' \n",
    "\n",
    "# train model\n",
    "cv = GridSearchCV(pipeline, param_grid, n_jobs=4, cv=folds, verbose=1, return_train_score=True, refit=True)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# write out model\n",
    "#make sure student data is filled in to give the file a speaking name\n",
    "assert student_id is not None and student_lastname is not None and student_firstname is not None, 'Please fill in your Name and Student Id'\n",
    "\n",
    "submission_dirname = 'submission'\n",
    "if use_timestamps:\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "    filename_model = os.path.join(submission_dirname,f'stage1_model_{student_id}_{student_lastname}_{student_firstname}_{timestamp}.pkl')\n",
    "    filename_submission =  os.path.join(submission_dirname,f'stage1_{submission_prefix}_{student_id}_{student_lastname}_{student_firstname}_{timestamp}.csv')\n",
    "else:\n",
    "    filename_model = os.path.join(submission_dirname,f'stage1_model_{student_id}_{student_lastname}_{student_firstname}.pkl')\n",
    "    filename_submission =  os.path.join(submission_dirname,f'stage1_{submission_prefix}_{student_id}_{student_lastname}_{student_firstname}.csv')\n",
    "\n",
    "if not os.path.exists(submission_dirname):\n",
    "    os.mkdir(submission_dirname)    \n",
    "\n",
    "with open(filename_model,'wb') as file:\n",
    "    pickle.dump(cv,file)\n",
    "    \n",
    "prediction = utils_for_students.label_encoder().inverse_transform(cv.best_estimator_.predict(X_test))\n",
    "utils_for_students.create_submission_file(filename_submission,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Printing scores\n",
    "Here we simply extract a bit more information about the individual scores obtained by the classifers we trained to fit the individual folds. Maybe a few plots may be useful to better understand what your classifier is doing? \n",
    "\n",
    "**Feel free to add as many cells as you like as long as you leave the locked training cell as-is, and only use models and submissions that have been exported by that cell. Good luck with the exercise!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cv.cv_results_\n",
    "mean_train_score = results['mean_train_score'][cv.best_index_]\n",
    "std_train_score = results['std_train_score'][cv.best_index_]\n",
    "mean_cv_score = results['mean_test_score'][cv.best_index_]\n",
    "std_cv_score = results['std_test_score'][cv.best_index_]\n",
    "\n",
    "print('Training accuracy {} +/- {}'.format(mean_train_score, std_train_score))\n",
    "print('Cross-validation accuracy: {} +/- {}'.format(mean_cv_score, std_cv_score))\n",
    "\n",
    "print('Best estimator:')\n",
    "print(cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b96519fadf0e27573d7db6a7c141d11ba6990cab00b556a7220b1bf1bdbac7f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
